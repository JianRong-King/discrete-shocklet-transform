#!/usr/bin/env python


import argparse
import multiprocessing
import pathlib

import numpy as np

import discrete_shocklets.utils
from discrete_shocklets import shocklets


def parse_args():
    parser = argparse.ArgumentParser(
        description='Runs the Shocklet Transform And Ranking (STAR) algorithm on data',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        '-i',
        '--input',
        type=str,
        help='Path to files on which to run the algorithm. '
             'The files should be in row-major order. '
             'That is, they should be a N_variable x T matrix, where T is the number of timesteps. '
             'This algorithm multiprocesses over files so if your time series are very long it could '
             'be significantly faster to break them up into multiple files, one time series per file. '
             '(This is true only if you have many cores on which processing can occur in parallel.)',
        default='.',
    )
    parser.add_argument(
        '-e',
        '--ending',
        type=str,
        help='Ending of files on which to run algorithm. Must be readable to numpy.genfromtxt()',
        default='csv'
    )
    parser.add_argument(
        '-d',
        '--delimiter',
        type=str,
        help='Delimiter of entries in files',
        default=',',
    )
    parser.add_argument(
        '-o',
        '--output',
        type=str,
        help='Path to which to save output',
        default='../out',
    )
    parser.add_argument(
        '-k',
        '--kernel',
        type=str,
        help='Kernel function to use. '
             'Must be in dir(shocklets). '
             'Pre-written options (no need to write code) are: '
             'haar, which is the Haar wavelet and looks for pure level changes; '
             'power_zero_cusp, which is the building block for other cusp kernels and looks for power (monomial) growth followed by an abrupt drop to constant low levels; '
             'power_cusp, which is a power cusp shape; exp_zero_cusp, which is like power_zero_cusp except with exponential growth; '
             'and exp_cusp, which is like power_cusp except with exponential growth. '
             'Combining these kernels with a reflection (see the -r option) is probably enough to look for most interesting behavior. ',
        default='power_cusp',
    )
    parser.add_argument(
        '-r',
        '--reflection',
        type=lambda x: int(x) % 4,
        help='Element of the reflection group R_4 to use. Computed mod 4.',
        default=0,
    )
    parser.add_argument(
        '-b',
        '--bvalue',
        type=float,
        help='Multiplier for std dev in classification.',
        default=0.75,
    )
    parser.add_argument(
        '-g',
        '--geval',
        type=float,
        help='Threshold for window construction.',
        default=0.5,
    )
    parser.add_argument(
        '-l',
        '--lookback',
        type=int,
        help='Number of indices to look back for window construction.',
        default=0,
    )
    parser.add_argument(
        '-w',
        '--weighting',
        type=str,
        help='Method for weighting of cusp indicator functions. '
             'max_change uses the dynamic range of the original series within each window. '
             'max_rel_change computes max_change on the array of log returns of the original time series.',
        default='max_change',
        choices={'max_change', 'max_rel_change'},
    )
    parser.add_argument(
        '-wmin',
        '--wmin',
        type=int,
        help='Smallest kernel size.',
        default=10,
    )
    parser.add_argument(
        '-wmax',
        '--wmax',
        type=int,
        help='Largest kernel size. Defaults to min{500, 1/2 length of time series}.',
        default=None,
    )
    parser.add_argument(
        '-nw',
        '--nw',
        type=int,
        help='Number of kernels to use. Ideally (wmax - wmin) / nw would be an integer.',
        default=100,
    )
    parser.add_argument(
        '-s',
        '--savespec',
        type=str,
        help='Spec for saving. Options are: '
             'cc, to save cusplet transform; '
             'indic, to save indicator function;'
             'windows, to save anomalous windows; '
             'weighted, to save weighted indicator function; '
             'all, to save everything. '
             'Files are saved in the numpy compressed archive format (.npz files).',
        default='all',
    )
    parser.add_argument(
        '-norm',
        '--norm',
        type=bool,
        help='Whether or not to normalize series to be wide-sense stationary '
             'with intertemporal zero mean and unit variance.',
        default=False,
    )

    return parser.parse_known_args()


def _process(
        data,
        kernel,
        reflection,
        wmin,
        wmax,
        nw,
        kernel_args,
        outdir,
        weighting,
        savespec,
        b,
        geval,
        nback,
        orig_fname,
        norm
):
    """Computes the shocklet (cusplet) transform on time series data.

    Computes the transform on each row of the passed data.
    The collection of cusplet transforms will be of shape (data.shape[0], nw, data.shape[1]).
    """
    if len(data.shape) < 2:
        data = data.reshape(1, data.shape[0])

    if wmax is None:
        nt = data.shape[1]
        wmax = min(500, int(0.5 * nt))

    kernel = getattr(shocklets, kernel)
    weighting = getattr(shocklets, weighting)
    widths = np.linspace(wmin, wmax, nw).astype(int)

    for i, row in enumerate(data):
        if norm:
            row = discrete_shocklets.utils.normalize(row)
        try:
            cc, _ = shocklets.cusplet(
                row,
                kernel,
                widths,
                k_args=kernel_args,
                reflection=reflection
            )
        except Exception as e:
            print(f'Error occurred in computation of shocklet transform of {orig_fname}')
            print(f'Error: {e}')
            return

        if savespec == 'cc':
            np.savez_compressed(
                outdir + f'{orig_fname}-row{i}',
                cc=cc,
            )
        else:
            extrema, sum_cc, gearray = shocklets.classify_cusps(
                cc,
                b=b,
                geval=geval
            )
            if savespec == 'indic':
                np.savez_compressed(
                    outdir + f'{orig_fname}-row{i}',
                    indic=sum_cc,
                )
            elif savespec in ['windows', 'weighted', 'all']:
                windows = shocklets.make_components(
                    gearray,
                    scan_back=nback
                )
                if savespec == 'windows':
                    np.savez_compressed(
                        outdir + f'{orig_fname}-row{i}',
                        windows=windows,
                    )

                else:
                    weighted_sumcc = np.copy(sum_cc)

                    for window in windows:
                        weighted_sumcc[window] *= weighting(row[window])

                    if savespec == 'weighted':
                        np.savez_compressed(
                            outdir + f'{orig_fname}-row{i}',
                            weighted_indic=weighted_sumcc,
                        )
                    else:
                        np.savez_compressed(
                            outdir + '/' + f'{orig_fname}-row{i}',
                            cc=cc,
                            indic=sum_cc,
                            windows=windows,
                            weighted_indic=weighted_sumcc,
                        )


def _mp_process(fname, args, kernel_args):
    if args.delimiter == 'none':
        data = np.genfromtxt(fname)
    else:
        data = np.genfromtxt(fname, delimiter=args.delimiter)

    _process(
        data,
        args.kernel,
        args.reflection,
        args.wmin,
        args.wmax,
        args.nw,
        kernel_args,
        args.output,
        args.weighting,
        args.savespec,
        args.bvalue,
        args.geval,
        args.lookback,
        fname.stem,
        args.norm
    )


def main():
    args, kernel_args = parse_args()

    input_dir = pathlib.Path(args.input)
    if not input_dir.is_dir():
        FileNotFoundError(f'{args.input} does not exist or is not a directory')

    try:
        kernel_args_ = []
        for i, arg in enumerate(kernel_args):
            kernel_args_.append(float(arg))
        kernel_args = kernel_args_
    except ValueError:
        ValueError(f'Kernel argument {i} = {arg} can not be safely cast to float.')

    if args.savespec not in ['cc', 'windows', 'indic', 'all']:
        print(f'{args.savespec} not one of cc, windows, indic, or all. Defaulting to all.')
        args.savespec = 'all'

    if args.wmin >= args.wmax:
        ValueError(f'wmin ({args.wmin}) must be less than wmax ({args.wmax}).')

    pathlib.Path(args.output).mkdir(exist_ok=True, parents=True)

    fnames = sorted(input_dir.glob(f'*.{args.ending}'))
    if len(fnames) == 0:
        FileNotFoundError(f'There are no files with ending {args.ending} in {args.input}')

    elif len(fnames) == 1:
        _mp_process(
            fnames[0],
            args,
            kernel_args,
        )

    elif len(fnames) > 1:
        with multiprocessing.Pool(None) as pool:
            pool.starmap(
                _mp_process,
                zip(
                    fnames,
                    (args for _ in range(len(fnames))),
                    (kernel_args for _ in range(len(fnames)))
                )
            )


if __name__ == "__main__":
    main()
